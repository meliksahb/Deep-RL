{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMHUyHmJfhp/HKCr/iX+o/b",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/meliksahb/Deep-RL/blob/main/DRL_Atari.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "zdg_B2Mj_tuT"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DeepQNetwork(object):\n",
        "  def __init__(self, lr, n_actions, name, fc1_dims=256,\n",
        "               input_dims=(210,160,4), chkpt_dir='tmp/dqn'):\n",
        "    \"name of the network is important because we are gonna have           n\\\n",
        "    2 networks one to select an action and one to tell us the             n\\\n",
        "    value of an action                                                    n\\\n",
        "    n\\\n",
        "    fc1_dims= number of dimensions in the first fully connected layer     n\\\n",
        "    n\\\n",
        "    input_dims= input dimension of our env. for atari library in gym      n\\\n",
        "    all of the images have 210 by 160 resolution and we are going to      n\\\n",
        "    pass in a set of frames to give the agent a sense of motion we are    n\\\n",
        "    going to pass in 4 frames in particular. we will do some cropping     n\\\n",
        "    n\\\n",
        "    chkpt_dir = directory to save checkpoints of model\"\n",
        "\n",
        "    self.lr = lr\n",
        "    self.name = name\n",
        "    self.n_actions = n_actions\n",
        "    self.fc1_dims = fc1_dims\n",
        "    self. input_dims = input_dims\n",
        "\n",
        "    # tensorflow session this is what instantiates everything into the graph\n",
        "    # each network want to have its own\n",
        "    self.sess = tf.Session()\n",
        "\n",
        "    # add everything to the graph\n",
        "    self.build_network()\n",
        "\n",
        "    # once you have added everything to the graph, you have to initialize it\n",
        "    self.sess.run(tf.global_variables_initializer())\n",
        "\n",
        "    # save model\n",
        "    self.saver = tf.train.Saver()  # its going to train for quite some time\n",
        "    # so we are going to want to be able to save it as we go along because\n",
        "    # we have other stuff to do\n",
        "\n",
        "    # save checkpoint files\n",
        "    self.checkpoint_file = os.path.join(chkpt_dir, 'deepqnet.ckpt')\n",
        "\n",
        "    # keep track of the parameters for each particular network\n",
        "    self.params = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\n",
        "                                    scope=self.name)\n",
        "    # we use this later when we copy one network to another\n",
        "\n",
        "  # build network\n",
        "  def build_net(self):\n",
        "\n",
        "    # encase everyting in a scope that is based on the network name\n",
        "    with tf.variable_scope(self.name):\n",
        "\n",
        "      # we are going to have olaceholder variables that tell us the inputs to\n",
        "      # our model. we are going to want to input the stack of images from the\n",
        "      # atari game, we want to input the actions that the agent took as well as\n",
        "      # the target value for the q network\n",
        "      self.input = tf.placeholder(tf.float32, shape=[None, *self.input_dims],\n",
        "                                                     name='inputs')\n",
        "      # this convention of naming placeholders and layers repeated because it\n",
        "      # makes debugging easier. if you get an error it will tell you the\n",
        "      # variable or layer that caused the error\n",
        "\n",
        "\n",
        "      # one hot encoding of the actions\n",
        "      self. actions = tf.placeholder(tf.float32,shape=[None, self.n_actions],\n",
        "                                     name='action_taken')\n",
        "\n",
        "      # same thing for the q_target\n",
        "      self.q_target = tf.placeholder(tf.float32, shape=[None, self.n_actions])\n",
        "\n",
        "      # convention of using None as the first parameter in the shape allows you\n",
        "      # to train a batch of stuff and thats the important because in virtually\n",
        "      # every deep learning application you want to pass in a batch of info\n",
        "      # in this case we are going to be passing in batches of stacked frames\n",
        "\n",
        "\n",
        "      # start build conv layers\n",
        "      conv1 = tf.layers.conv2d(inputs=self.input, filters=32,\n",
        "                               kernel_size=(8,8), strides=4, name='conv1',\n",
        "                               kernel_initializer=tf.variance_scaling_initializer(scale=2))\n",
        "\n",
        "      # activate that with a relu func\n",
        "      conv1_activated = tf.nn.relu(conv1)\n",
        "\n",
        "      conv2 = tf.layers.conv2d(inputs=conv1_activated, filters=64,\n",
        "                               kernel_size=(4,4), strides=2, name='conv2',\n",
        "                               kernel_initializer=tf.variance_scaling_initializer(scale=2))\n",
        "\n",
        "      conv2_activated = tf.nn.relu(conv2)\n",
        "\n",
        "      conv3 = tf.layers.conv2d(inputs=conv2_activated, filters=128,\n",
        "                               kernel_size=(3,3), strides=1, name='conv3',\n",
        "                               kernel_initializer=tf.variance_scaling_initializer(scale=2))\n",
        "\n",
        "      conv3_activated = tf.nn.relu(conv3)\n",
        "\n",
        "      # flatten all of them and pass them through a dense network to get q values\n",
        "      # or the values of each state action pair\n",
        "      flat = tf.layers.flatten(conv3_activated)\n",
        "\n",
        "      dense1 = tf.layers.dense(flat, units=self.fc1_dims,\n",
        "                               activation=tf.nn.relu,\n",
        "                               kernel_initializer=tf.variance_scaling_initializer(scale=2))\n",
        "\n",
        "      # determine q values\n",
        "      # q values and q learning just refers t the value of a state action pair\n",
        "      # this will be output of our neural network (1 output for each action)\n",
        "      self.Q_values = tf.layers.dense(dense1, units=self.n_actions,\n",
        "                                      kernel_initializer=tf.variance_scaling_initializer(scale=2))\n",
        "\n",
        "      # we are not activating that yet. we want to just get linear activation of\n",
        "      # the output of out network\n",
        "\n",
        "      # actual value of q for each action (Actions is a placeholder)\n",
        "      self.q = tf.reduce_sum(tf.multiply(self.Q_values, self.actions))\n",
        "\n",
        "      # squared difference between the q value of the network outputs and q target\n",
        "      self.loss = tf.reduce_mean(tf.square(self.q - self.q_target))\n",
        "\n",
        "      # the way q learning works is that at each time step it's a form of temporal\n",
        "      # difference learning. so every time step, it learns and it says 'hey, I\n",
        "      # took some action, what was the maximal action I could have taken' and then\n",
        "      # it takes the delta between whatever action it took and the maximal action\n",
        "      # and uses that to update the neural network as its loss function\n",
        "\n",
        "      # training\n",
        "      self.train_op = tf.train.AdamOptimizer(self.lr).minimize(self.loss)\n",
        "\n",
        "  # save model\n",
        "  def load_checkpoint(self):\n",
        "    'reason is that these models take a notoriously long time to train and so n\\\n",
        "    we may want to start and stop as we go along'\n",
        "\n",
        "    print('... loading checkpoint ...')\n",
        "\n",
        "    self.saver.restore(self.sess, self.checkpoint_file)\n",
        "    # it will look in the checkpoint file and load up the graph from that file\n",
        "    # and save it and load it into the graph of the current session\n",
        "\n",
        "  def save_checkpoint(self):\n",
        "\n",
        "    print('... saving checkpoint ...')\n",
        "\n",
        "    self.saver.save(self.sess, self.checkpoint_file)\n",
        "    # takes the current session and opposite to a file\n",
        "\n",
        "# summary\n",
        "# this deep q network takes a batch of images from the env in this case breakout\n",
        "# passes it through CNN to do the feature selection that passes it through a fully\n",
        "# connected layer to detern≈übe the value of each given action and then uses the\n",
        "# maximum value of the next action to determine its loss function and perform\n",
        "# training on that network via back propagation\n",
        "\n",
        "  # agent that includes everything else all of the learnings, memories etc.\n",
        "  class Agent(object):\n",
        "    def __init__(self, alpha, gamma, mem_size, n_actions, epsilon, batch_size,\n",
        "                 replace_target=5000, input_dims=(210, 64, 4),\n",
        "                 q_next='tmp/q_next', q_eval='tmp/q_eval'):\n",
        "\n",
        "    # alpha: learning rate\n",
        "    # gamma: discount factor\n",
        "    # epsilon: determines how often it takes a random action\n",
        "    # replace_target: how often we want to replace our target network\n",
        "\n",
        "    # one network says action to take other one says value of that action\n",
        "\n",
        "        # when take random actions we will need to know the action space\n",
        "        # and we need to know the number of actions\n",
        "        self.n_actions = n_actions\n",
        "        self.action_space = [i for i in range(self.n_actions)]\n",
        "\n",
        "        # discount factor: tells the agent how much it wants to discount future rewards\n",
        "        self.gamma = gamma\n",
        "\n",
        "        # memory size: tells us how many transitions to store in memory\n",
        "        self.mem_size = mem_size\n",
        "\n",
        "\n",
        "        self.epsilon = epsilon\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        self.mem_cntr = 0\n",
        "\n",
        "        self.replace_target = replace_target\n",
        "\n",
        "        # we need our network to tell the agent the value of the next action\n",
        "        self.q_next = DeepQNetwork(alpha, n_actions, input_dims=input_dims,\n",
        "                                   name='q_next', chkpt_dir=q_next_dir)\n",
        "\n",
        "        self.q_eval = DeepQNetwork(alpha, n_actions, input_dims=input_dims,\n",
        "                                   name='q_eval', chkpt_dir=q_eval_dir)\n",
        "\n",
        "        # we have our 2 networks the next thing we need is a memory so q learning\n",
        "        # works by saving the state, action, reward and new state transitions in\n",
        "        # its memory.\n",
        "\n",
        "        self.state_memory = np.zeros((self.mem_size, *input_dims))\n",
        "        # this will save a set of four transitions four frames stacked\n",
        "        # four frames by number of memories\n",
        "\n",
        "        self.new_state_memory = np.zeros((self.mem_size, *input_dims))\n",
        "\n",
        "        # we lso need an action memory. this will store the one hot encoding of\n",
        "        # our actions. to save ram, save that as int8\n",
        "        self.action_memory = np.zeros((self.mem_size, n_actions, dtype=np.int8))\n",
        "\n",
        "        self.reward_memory = np.zeros(self.mem_size)\n",
        "\n",
        "        # terminal memory just saves the memory of the done flex and to save ram\n",
        "        # we will save that one as int8\n",
        "        self.terminal_memory = np.zeros(self.mem_size, dtype=np.int8)\n",
        "\n",
        "    def store_transition(self, state, action, reward, state_, terminal):\n",
        "      # agent has some fixed memeory signs we want to fill up to that memory and\n",
        "      # then when we exceed it we just want to go back to the beginning and start\n",
        "      # overriding it.\n",
        "\n",
        "      # counter that keeps track of the number of memories that it has stored\n",
        "      index = self.mem_cntr % self.mem_size\n",
        "\n",
        "      self.state_memory[index] = state\n",
        "\n",
        "      # do one hot encoding\n",
        "      actions = np.zeros(self.n_actions)\n",
        "\n",
        "      # we pass in the action it will just be an integer\n",
        "      actions[action] = 1.0\n",
        "\n",
        "      # making an array of zeros and setting the index of the action you took to\n",
        "      # one is one hot encoding\n",
        "\n",
        "      self.action_memory[index] = actions\n",
        "      self.reward_memory[index] = reward\n",
        "      self.new_state_memory[index] = state_\n",
        "      self.terminal_memory[index] = 1 - terminal\n",
        "\n",
        "      self.mem_cntr += 1\n",
        "\n",
        "      # deep q learning relies on what is called epsilon greedy. so epsilon is a\n",
        "      # parameter that tells it how often to choose a random action we are going\n",
        "      # to dk_epsilon over time the agent will start out acting purely randomly\n",
        "      # for many many hundreds of games and eventually the random factor will\n",
        "      # start decreasing over time and the agent will take more and more greedy\n",
        "      # actions. greedy action is choosing the action that has the highest value\n",
        "      # of the next state.\n",
        "\n",
        "    def choose_action(self,state):\n",
        "      rand = np.random.random()\n",
        "\n",
        "      # select an action at random from the agents action space\n",
        "      if rand < self.epsilon:\n",
        "\n",
        "        action = np.random.choice(self.action_space)\n",
        "\n",
        "      else:\n",
        "\n",
        "        # if we are going to take a greedy action, then we need to actually find\n",
        "        # out what out next highest valued action is so we need to use our evaluation\n",
        "        # network to run\n",
        "        actions = self.q_eval.sess.run(self.q_eval.Q_values,\n",
        "                                       feed_dict={self.q_eval.input: state})\n",
        "        # current state as the q evaluation network input\n",
        "\n",
        "        # take max action\n",
        "        action = np.argmax(actions)\n",
        "\n",
        "        return action\n",
        "\n",
        "# learning part\n",
        "# learning has many parts to it. basic idea is first thing we are going to do is\n",
        "# check to see if we want to update the value of our target network and if its\n",
        "# time to do that we are going to go ahead and do that.\n",
        "# next thing we are going to do is select a batch of random memories the most\n",
        "# important thing here is that these memories are non-sequential. if you choose\n",
        "# sequential memories then the agent will get trapped in little parameter space\n",
        "# and what you'll get is oscillations and performance over time.\n",
        "# to actually have robust learning you want to select different transitions over\n",
        "# the entirety of the memory.\n",
        "# then you have to calculate the value of the current action as well as the next\n",
        "# maximum action and then you plug that into the bellman eq for the q learning\n",
        "# algo and run your update func on your loss.\n",
        "\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "EczxN8qS_7nn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}