{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPAyPvVmjUx0h2MLoi/rxKT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/meliksahb/Deep-RL/blob/main/DRL_Atari.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "zdg_B2Mj_tuT"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DeepQNetwork(object):\n",
        "  def __init__(self, lr, n_actions, name, fc1_dims=256,\n",
        "               input_dims=(210,160,4), chkpt_dir='tmp/dqn'):\n",
        "    \"name of the network is important because we are gonna have           n\\\n",
        "    2 networks one to select an action and one to tell us the             n\\\n",
        "    value of an action                                                    n\\\n",
        "    n\\\n",
        "    fc1_dims= number of dimensions in the first fully connected layer     n\\\n",
        "    n\\\n",
        "    input_dims= input dimension of our env. for atari library in gym      n\\\n",
        "    all of the images have 210 by 160 resolution and we are going to      n\\\n",
        "    pass in a set of frames to give the agent a sense of motion we are    n\\\n",
        "    going to pass in 4 frames in particular. we will do some cropping     n\\\n",
        "    n\\\n",
        "    chkpt_dir = directory to save checkpoints of model\"\n",
        "\n",
        "    self.lr = lr\n",
        "    self.name = name\n",
        "    self.n_actions = n_actions\n",
        "    self.fc1_dims = fc1_dims\n",
        "    self. input_dims = input_dims\n",
        "\n",
        "    # tensorflow session this is what instantiates everything into the graph\n",
        "    # each network want to have its own\n",
        "    self.sess = tf.Session()\n",
        "\n",
        "    # add everything to the graph\n",
        "    self.build_network()\n",
        "\n",
        "    # once you have added everything to the graph, you have to initialize it\n",
        "    self.sess.run(tf.global_variables_initializer())\n",
        "\n",
        "    # save model\n",
        "    self.saver = tf.train.Saver()  # its going to train for quite some time\n",
        "    # so we are going to want to be able to save it as we go along because\n",
        "    # we have other stuff to do\n",
        "\n",
        "    # save checkpoint files\n",
        "    self.checkpoint_file = os.path.join(chkpt_dir, 'deepqnet.ckpt')\n",
        "\n",
        "    # keep track of the parameters for each particular network\n",
        "    self.params = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\n",
        "                                    scope=self.name)\n",
        "    # we use this later when we copy one network to another\n",
        "\n",
        "  # build network\n",
        "  def build_net(self):\n",
        "\n",
        "    # encase everyting in a scope that is based on the network name\n",
        "    with tf.variable_scope(self.name):\n",
        "\n",
        "      # we are going to have olaceholder variables that tell us the inputs to\n",
        "      # our model. we are going to want to input the stack of images from the\n",
        "      # atari game, we want to input the actions that the agent took as well as\n",
        "      # the target value for the q network\n",
        "      self.input = tf.placeholder(tf.float32, shape=[None, *self.input_dims],\n",
        "                                                     name='inputs')\n",
        "      # this convention of naming placeholders and layers repeated because it\n",
        "      # makes debugging easier. if you get an error it will tell you the\n",
        "      # variable or layer that caused the error\n",
        "\n",
        "\n",
        "      # one hot encoding of the actions\n",
        "      self. actions = tf.placeholder(tf.float32,shape=[None, self.n_actions],\n",
        "                                     name='action_taken')\n",
        "\n",
        "      # same thing for the q_target\n",
        "      self.q_target = tf.placeholder(tf.float32, shape=[None, self.n_actions])\n",
        "\n",
        "      # convention of using None as the first parameter in the shape allows you\n",
        "      # to train a batch of stuff and thats the important because in virtually\n",
        "      # every deep learning application you want to pass in a batch of info\n",
        "      # in this case we are going to be passing in batches of stacked frames\n",
        "\n",
        "\n",
        "      # start build conv layers\n",
        "      conv1 = tf.layers.conv2d(inputs=self.input, filters=32,\n",
        "                               kernel_size=(8,8), strides=4, name='conv1',\n",
        "                               kernel_initializer=tf.variance_scaling_initializer(scale=2))\n",
        "\n",
        "      # activate that with a relu func\n",
        "      conv1_activated = tf.nn.relu(conv1)\n",
        "\n",
        "      conv2 = tf.layers.conv2d(inputs=conv1_activated, filters=64,\n",
        "                               kernel_size=(4,4), strides=2, name='conv2',\n",
        "                               kernel_initializer=tf.variance_scaling_initializer(scale=2))\n",
        "\n",
        "      conv2_activated = tf.nn.relu(conv2)\n",
        "\n",
        "      conv3 = tf.layers.conv2d(inputs=conv2_activated, filters=128,\n",
        "                               kernel_size=(3,3), strides=1, name='conv3',\n",
        "                               kernel_initializer=tf.variance_scaling_initializer(scale=2))\n",
        "\n",
        "      conv3_activated = tf.nn.relu(conv3)\n",
        "\n",
        "      # flatten all of them and pass them through a dense network to get q values\n",
        "      # or the values of each state action pair\n",
        "      flat = tf.layers.flatten(conv3_activated)\n",
        "\n",
        "      dense1 = tf.layers.dense(flat, units=self.fc1_dims,\n",
        "                               activation=tf.nn.relu,\n",
        "                               kernel_initializer=tf.variance_scaling_initializer(scale=2))\n",
        "\n",
        "      # determine q values\n",
        "      # q values and q learning just refers t the value of a state action pair\n",
        "      # this will be output of our neural network (1 output for each action)\n",
        "      self.Q_values = tf.layers.dense(dense1, units=self.n_actions,\n",
        "                                      kernel_initializer=tf.variance_scaling_initializer(scale=2))\n",
        "\n",
        "      # we are not activating that yet. we want to just get linear activation of\n",
        "      # the output of out network\n",
        "\n",
        "      # actual value of q for each action (Actions is a placeholder)\n",
        "      self.q = tf.reduce_sum(tf.multiply(self.Q_values, self.actions))\n",
        "\n",
        "      # squared difference between the q value of the network outputs and q target\n",
        "      self.loss = tf.reduce_mean(tf.square(self.q - self.q_target))\n",
        "\n",
        "      # the way q learning works is that at each time step it's a form of temporal\n",
        "      # difference learning. so every time step, it learns and it says 'hey, I\n",
        "      # took some action, what was the maximal action I could have taken' and then\n",
        "      # it takes the delta between whatever action it took and the maximal action\n",
        "      # and uses that to update the neural network as its loss function\n",
        "\n",
        "      # training\n",
        "      self.train_op = tf.train.AdamOptimizer(self.lr).minimize(self.loss)\n",
        "\n",
        "  # save model\n",
        "  def load_checkpoint(self):\n",
        "    'reason is that these models take a notoriously long time to train and so n\\\n",
        "    we may want to start and stop as we go along'\n",
        "\n",
        "    print('... loading checkpoint ...')\n",
        "\n",
        "    self.saver.restore(self.sess, self.checkpoint_file)\n",
        "    # it will look in the checkpoint file and load up the graph from that file\n",
        "    # and save it and load it into the graph of the current session\n",
        "\n",
        "  def save_checkpoint(self):\n",
        "\n",
        "    print('... saving checkpoint ...')\n",
        "\n",
        "    self.saver.save(self.sess, self.checkpoint_file)\n",
        "    # takes the current session and opposite to a file\n",
        "\n",
        "# summary\n",
        "# this deep q network takes a batch of images from the env in this case breakout\n",
        "# passes it through CNN to do the feature selection that passes it through a fully\n",
        "# connected layer to deternşbe the value of each given action and then uses the\n",
        "# maximum value of the next action to determine its loss function and perform\n",
        "# training on that network via back propagation\n",
        "\n",
        "  # agent that includes everything else all of the learnings, memories etc.\n",
        "  class Agent(object):\n",
        "    def __init__(self, alpha, gamma, mem_size, n_actions, epsilon, batch_size,\n",
        "                 replace_target=5000, input_dims=(210, 64, 4),\n",
        "                 q_next='tmp/q_next', q_eval='tmp/q_eval'):\n",
        "\n",
        "    # alpha: learning rate\n",
        "    # gamma: discount factor\n",
        "    # epsilon: determines how often it takes a random action\n",
        "    # replace_target: how often we want to replace our target network\n",
        "\n",
        "    # one network says action to take other one says value of that action\n",
        "\n",
        "        # when take random actions we will need to know the action space\n",
        "        # and we need to know the number of actions\n",
        "        self.n_actions = n_actions\n",
        "        self.action_space = [i for i in range(self.n_actions)]\n",
        "\n",
        "        # discount factor: tells the agent how much it wants to discount future rewards\n",
        "        self.gamma = gamma\n",
        "\n",
        "        # memory size: tells us how many transitions to store in memory\n",
        "        self.mem_size = mem_size\n",
        "\n",
        "\n",
        "        self.epsilon = epsilon\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        self.mem_cntr = 0\n",
        "\n",
        "        self.replace_target = replace_target\n",
        "\n",
        "        # we need our network to tell the agent the value of the next action\n",
        "        self.q_next = DeepQNetwork(alpha, n_actions, input_dims=input_dims,\n",
        "                                   name='q_next', chkpt_dir=q_next_dir)\n",
        "\n",
        "        self.q_eval = DeepQNetwork(alpha, n_actions, input_dims=input_dims,\n",
        "                                   name='q_eval', chkpt_dir=q_eval_dir)\n",
        "\n",
        "        # we have our 2 networks the next thing we need is a memory so q learning\n",
        "        # works by saving the state, action, reward and new state transitions in\n",
        "        # its memory.\n",
        "\n",
        "        self.state_memory = np.zeros((self.mem_size, *input_dims))\n",
        "        # this will save a set of four transitions four frames stacked\n",
        "        # four frames by number of memories\n",
        "\n",
        "        self.new_state_memory = np.zeros((self.mem_size, *input_dims))\n",
        "\n",
        "        # we lso need an action memory. this will store the one hot encoding of\n",
        "        # our actions. to save ram, save that as int8\n",
        "        self.action_memory = np.zeros((self.mem_size, n_actions, dtype==np.int8))\n",
        "\n",
        "        self.reward_memory = np.zeros(self.mem_size)\n",
        "\n",
        "        # terminal memory just saves the memory of the done flex and to save ram\n",
        "        # we will save that one as int8\n",
        "        self.terminal_memory = np.zeros(self.mem_size, dtype=np.int8)\n",
        "\n",
        "    def store_transition(self, state, action, reward, state_, terminal):\n",
        "      # agent has some fixed memeory signs we want to fill up to that memory and\n",
        "      # then when we exceed it we just want to go back to the beginning and start\n",
        "      # overriding it.\n",
        "\n",
        "      # counter that keeps track of the number of memories that it has stored\n",
        "      index = self.mem_cntr % self.mem_size\n",
        "\n",
        "      self.state_memory[index] = state\n",
        "\n",
        "      # do one hot encoding\n",
        "      actions = np.zeros(self.n_actions)\n",
        "\n",
        "      # we pass in the action it will just be an integer\n",
        "      actions[action] = 1.0\n",
        "\n",
        "      # making an array of zeros and setting the index of the action you took to\n",
        "      # one is one hot encoding\n",
        "\n",
        "      self.action_memory[index] = actions\n",
        "      self.reward_memory[index] = reward\n",
        "      self.new_state_memory[index] = state_\n",
        "      self.terminal_memory[index] = 1 - terminal\n",
        "\n",
        "      self.mem_cntr += 1\n",
        "\n",
        "      # deep q learning relies on what is called epsilon greedy. so epsilon is a\n",
        "      # parameter that tells it how often to choose a random action we are going\n",
        "      # to dk_epsilon over time the agent will start out acting purely randomly\n",
        "      # for many many hundreds of games and eventually the random factor will\n",
        "      # start decreasing over time and the agent will take more and more greedy\n",
        "      # actions. greedy action is choosing the action that has the highest value\n",
        "      # of the next state.\n",
        "\n",
        "    def choose_action(self,state):\n",
        "      rand = np.random.random()\n",
        "\n",
        "      # select an action at random from the agents action space\n",
        "      if rand < self.epsilon:\n",
        "\n",
        "        action = np.random.choice(self.action_space)\n",
        "\n",
        "      else:\n",
        "\n",
        "        # if we are going to take a greedy action, then we need to actually find\n",
        "        # out what out next highest valued action is so we need to use our evaluation\n",
        "        # network to run\n",
        "        actions = self.q_eval.sess.run(self.q_eval.Q_values,\n",
        "                                       feed_dict={self.q_eval.input: state})\n",
        "        # current state as the q evaluation network input\n",
        "\n",
        "        # take max action\n",
        "        action = np.argmax(actions)\n",
        "\n",
        "        return action\n",
        "\n",
        "# learning part\n",
        "# learning has many parts to it. basic idea is first thing we are going to do is\n",
        "# check to see if we want to update the value of our target network and if its\n",
        "# time to do that we are going to go ahead and do that.\n",
        "# next thing we are going to do is select a batch of random memories the most\n",
        "# important thing here is that these memories are non-sequential. if you choose\n",
        "# sequential memories then the agent will get trapped in little parameter space\n",
        "# and what you'll get is oscillations and performance over time.\n",
        "# to actually have robust learning you want to select different transitions over\n",
        "# the entirety of the memory.\n",
        "# then you have to calculate the value of the current action as well as the next\n",
        "# maximum action and then you plug that into the bellman eq for the q learning\n",
        "# algo and run your update func on your loss.\n",
        "\n",
        "    def learn(self):\n",
        "\n",
        "      # if its time to replace out target network\n",
        "      if self.mem_cntr % self.replace_target == 0:\n",
        "        self.update_graph()  # we will write the update graph func momentarily\n",
        "\n",
        "      # find out where our memory ends\n",
        "      max_mem = self.mem_cntr if self.mem.cntr < self.mem_size else self.mem_size\n",
        "      # this will allows us to randomly sample a subset of the memory\n",
        "\n",
        "      # this will give us a random choice in the range maximum of batch size\n",
        "      batch = np.random.choice(max_mem, self.batch_size)\n",
        "\n",
        "      # these are just state transitions\n",
        "      state_batch = self.state_memory[batch]\n",
        "\n",
        "      # actions we took we store these as one-hot encoding\n",
        "      action_batch = self.action_memory[batch]\n",
        "\n",
        "      # so we need to go back into integer encoding\n",
        "      action_values = np.array([0, 1, 2], dtype=np.int8)\n",
        "\n",
        "      # simplest way to do that is numpy.dot operation to just multiply 2 vector\n",
        "      action_indices = np.dot(action_batch, action_values)\n",
        "\n",
        "      reward_batch = self.reward_memory[batch]\n",
        "\n",
        "      terminal_batch = self.terminal_memory[batch]\n",
        "\n",
        "      # we need to calculate the values of the current set of states as well as\n",
        "      # these set of next states\n",
        "\n",
        "      q_eval = self.q_eval.sess.run(self.q_eval.Q_values,\n",
        "                                    feed_dict={self.q_eval.input: state_batch})\n",
        "\n",
        "      # this will take the set of next states the transitions\n",
        "      q_next = self.q_next.sess.run(self.q_next.Q_values,\n",
        "                                    feed_dict={self.q_next.input: new_state_batch})\n",
        "\n",
        "      # copy eval network because we want the loss for all of the non-optimal\n",
        "      # actions to be zero\n",
        "      q_target = q_eval.copy()\n",
        "\n",
        "      # calculate the value of q target for all of these states in the batch for\n",
        "      # the actions we actually took\n",
        "      q_target[:, action_] = reward_batch + \\\n",
        "                              self.gamma * np.max(q_next, axis=1) * terminal_batch\n",
        "      # reason is if the next state is the end of the episode you just want to\n",
        "      # have the reward whereas if it is not a terminal state the next state then\n",
        "      # you want to actuallly take into account the discounted future rewards\n",
        "\n",
        "      # we need to feed all of this through our neural network\n",
        "      _ = self.q_eval.sess.run(self.q_eval.train_op,\n",
        "                               feed_dict={self.q_eval.input: state_batch,\n",
        "                                          self.q_eval.actions: action_batch,\n",
        "                                          self.q_eval.q_target: q_target})\n",
        "\n",
        "      # handle the prospect of decreasing epsilon over time (epsilon is random\n",
        "      # factor that tells the agent how often to take a random action)\n",
        "      # goal of learning is to eventually take the best possible actions so you\n",
        "      # want to decrease that epsilon over time.\n",
        "\n",
        "      # we handle that by allowing the agent to play some number moves randomly\n",
        "      # so we will 100 000 moves randomly\n",
        "      if self.mem_cntr > 100000:\n",
        "\n",
        "        # dictate some min. value because you never wanted to do purely greedy\n",
        "        # actions because you never know şf your estimates are off so you always\n",
        "        # wanted to do pruely greedy actions because you never know if your\n",
        "        # estimates are off. so you always want to be exploring to make sure\n",
        "        # estimates are not off\n",
        "        if self.epsilon > 0.01:\n",
        "\n",
        "          # you can decrease epsilon over time any number of ways. you can do it\n",
        "          # linearly, use square roots, multiply by number etc.\n",
        "          self.epsilon *= 0.999995\n",
        "          # that will give you a really slow decrease of epsilon over time so agent\n",
        "          # takes a lot of random actions and does a lot of exploration\n",
        "\n",
        "        elif self.epsilon < 0.01:\n",
        "\n",
        "          # if it tries to drop below 0.01 we are going to set it there\n",
        "          self.epsilon = 0.01\n",
        "\n",
        "          # that is the learning function\n",
        "\n",
        "    def save_models(self):\n",
        "      self.q_eval.save_checkpoint()\n",
        "      self.q_next.save_checkpoint()\n",
        "\n",
        "    def load_models(self):\n",
        "      self.q_eval.load_checkpoint()\n",
        "      self.q_next.load_checkpoint()\n",
        "\n",
        "    # we need a way of updating our graph so copy the evaluation network to the\n",
        "    # target network\n",
        "    def update_graph(self):\n",
        "      t_params = self.q_next.params\n",
        "      e_params = self.q_eval.params\n",
        "\n",
        "      for t, e in zip(t_params, e_params):\n",
        "\n",
        "        # reason this is non-trivial because you have to pass in a session and the\n",
        "        # decision of which session to use is non-trivial so you have to use the\n",
        "        # session for the values that trying to copy from not copy to. so if you\n",
        "        # had q next you would get an error\n",
        "        self.q_next.sess.run(tf.assign(t, e))"
      ],
      "metadata": {
        "id": "EczxN8qS_7nn"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install dqn_agent"
      ],
      "metadata": {
        "id": "COE2tHI20t4b",
        "outputId": "cea36550-e7ec-4ee0-b0c0-3c83b5727392",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement dqn_agent (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for dqn_agent\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# main function\n",
        "import gym\n",
        "from dqn_tf import DeepQNetwork, Agent\n",
        "import numpy as np\n",
        "from gym import wrappers"
      ],
      "metadata": {
        "id": "wnBOOqGsQhVk",
        "outputId": "a0607a5f-0aed-43d5-c4e7-ca0108c51b7c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 441
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "cannot import name 'DeepQNetwork' from 'tensorflow' (/usr/local/lib/python3.11/dist-packages/tensorflow/__init__.py)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-d6ed6d0c4a1b>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# main function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDeepQNetwork\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAgent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgym\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwrappers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'DeepQNetwork' from 'tensorflow' (/usr/local/lib/python3.11/dist-packages/tensorflow/__init__.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(observations):\n",
        "\n",
        "  # reason to do this is you dont need all of the image, dont need the score and\n",
        "  # dont need color. we just need one channel\n",
        "\n",
        "  # Ive figured it out if you take row 30 onward and all the columns then you will\n",
        "  # get a good image\n",
        "  return np.mean(observation[30:, :], axis=2).reshape(180,160,1)\n",
        "\n",
        "def stack_frames(stacked_frames, frame, buffer_size): # buffer_size tell how many frames to save\n",
        "\n",
        "  # stack the frames because agent cant get a sense of motion by looking at only\n",
        "  # one picture. worse yet openai atari library returns a sequence of frames\n",
        "  # where it could be a random number between 2,3 or 4 so to get a sense of motion\n",
        "\n",
        "  if stacked_frames is None:\n",
        "\n",
        "    # if this is the first frame initialize it\n",
        "    stacked_frames = np.zeros((buffer_size, *frame.shape))\n",
        "\n",
        "    for idx, _ in enumerate(stacked_frames):\n",
        "\n",
        "      # each row which corresponds to each image get assigned to a frame\n",
        "      stacked_frames[idx, :] = frame\n",
        "\n",
        "  else:\n",
        "\n",
        "    # otherwise its not the beginning of the episode and you want to pop off the\n",
        "    # bottom observation shift the set of frames down and append the new observation\n",
        "    # to the end so instead of 1, 2, 3, 4 it will be 2, 3, 4 and then frame 5\n",
        "\n",
        "    # this will shift everything down\n",
        "    stacked_frames[0:buffer_size-1, :] = stacked_frames[1:, :]\n",
        "\n",
        "    # this will append the current frame to the end of the stack\n",
        "    stacked_frames[buffer_size-1, :] = frame\n",
        "\n",
        "  stacked_frames = stacked_frames.reshape(1, *frame.shape[0:2], buffer_size)\n",
        "\n",
        "  return stacked_frames\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  env = gym.make('Breakout-v0')\n",
        "  load_checkpoint = False\n",
        "  agent = Agent(gamma=0.99, epsilon=1.0, alpha=0.00025, input_dims=(180,160,4),\n",
        "                n_actions=3, mem_size=25000, batch_size=32)\n",
        "\n",
        "  if load_checkpoint:\n",
        "    agent.load_models()\n",
        "\n",
        "  scores = []\n",
        "  numGames = 200\n",
        "  stack_size = 4\n",
        "  score = 0\n",
        "\n",
        "  # memory originally initialized with zeros. that is acceptable but another\n",
        "  # option is we can overwrite zeros with actual gameplay sampled from the env\n",
        "  # and the actions are just chosen randomly just to give the agen some idea of\n",
        "  # what is going on in the env\n",
        "  while agent.mem_cntr < 25000:\n",
        "\n",
        "    done = False\n",
        "\n",
        "    observation = env.reset()\n",
        "\n",
        "    observation = preprocess(observation)\n",
        "\n",
        "    stacked_frames = None\n",
        "\n",
        "    observation = stack_frames(stacked_frames, observation, stack_size)\n",
        "\n",
        "    while not done:\n",
        "\n",
        "      action = np.random.choice([0, 1, 2])\n",
        "\n",
        "      actions += 1\n",
        "\n",
        "      observation_, reward, done, info = env.step(action)\n",
        "\n",
        "      stacked_frames = stack_frames(stacked_frames, preprocess(observation_),\n",
        "                                    stack_size)\n",
        "\n",
        "      action -= 1\n",
        "\n",
        "      agent.store_transition(observation, action, reward,\n",
        "                             observation_, int(done))\n",
        "\n",
        "      observation = observation_\n",
        "\n",
        "    print('Done with random gamplay, game on.')\n",
        "\n",
        "    for i in range(numGames):\n",
        "\n",
        "      done = False\n",
        "\n",
        "      if i % 10 == 0 and i > 0:\n",
        "\n",
        "        avg_score = np.mean(scores[max(0, i-10):(i+1)])\n",
        "        print('episode: ', i,'score: ', score,\n",
        "              ' average score %.3f' % avg_score,\n",
        "              'epsilon %.3f' % agent.epsilon)\n",
        "\n",
        "        agent.save_models()\n",
        "\n",
        "      else:\n",
        "\n",
        "        print('episode: ', i,'score: ', score)\n",
        "\n",
        "      observation = env.reset()\n",
        "\n",
        "      observation = preprocess(observation)\n",
        "\n",
        "      stacked_frames = None\n",
        "\n",
        "      observation = stack_frames(stacked_frames, observation, stack_size)\n",
        "\n",
        "      while not done:\n",
        "\n",
        "        action = agent.choose_action(observation)\n",
        "\n",
        "        actions += 1\n",
        "\n",
        "        observation_, reward, done, info = env.step(action)\n",
        "\n",
        "        stacked_frames = stack_frames(stacked_frames, preprocess(observation_),\n",
        "                                      stack_size)\n",
        "\n",
        "        action -= 1\n",
        "\n",
        "        agent.store_transition(observation, action, reward,\n",
        "                              observation_, int(done))\n",
        "\n",
        "        observation = observation_\n",
        "\n",
        "        agent.learn()\n",
        "\n",
        "      score += reward\n",
        "\n",
        "      scores.append(score)"
      ],
      "metadata": {
        "id": "tdQ344utQvG0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}